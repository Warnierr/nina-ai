#!/bin/bash

# =============================================================================
# SCRIPT D'INSTALLATION AUTOMATIQUE NINA IA - VM UBUNTU
# =============================================================================
# Ce script installe tout ce dont Nina a besoin dans la VM Ubuntu
# SÃ©curisÃ©, optimisÃ©, et prÃªt pour l'IA locale

echo "ğŸš€ INSTALLATION NINA IA - VM UBUNTU SÃ‰CURISÃ‰E"
echo "=============================================="

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# =============================================================================
# 1. MISE Ã€ JOUR DU SYSTÃˆME
# =============================================================================
print_status "Mise Ã  jour du systÃ¨me Ubuntu..."
sudo apt update && sudo apt upgrade -y
print_success "SystÃ¨me mis Ã  jour"

# =============================================================================
# 2. INSTALLATION DES DÃ‰PENDANCES DE BASE
# =============================================================================
print_status "Installation des dÃ©pendances de base..."
sudo apt install -y \
    curl \
    wget \
    git \
    python3 \
    python3-pip \
    python3-venv \
    build-essential \
    software-properties-common \
    apt-transport-https \
    ca-certificates \
    gnupg \
    lsb-release \
    htop \
    nano \
    unzip \
    zip \
    jq

print_success "DÃ©pendances installÃ©es"

# =============================================================================
# 3. INSTALLATION OLLAMA (IA LOCALE SÃ‰CURISÃ‰E)
# =============================================================================
print_status "Installation d'Ollama (IA locale)..."
curl -fsSL https://ollama.com/install.sh | sh

# DÃ©marrer le service Ollama
sudo systemctl enable ollama
sudo systemctl start ollama

print_success "Ollama installÃ© et dÃ©marrÃ©"

# Attendre qu'Ollama soit prÃªt
print_status "Attente du dÃ©marrage d'Ollama..."
sleep 10

# =============================================================================
# 4. TÃ‰LÃ‰CHARGEMENT DES MODÃˆLES IA
# =============================================================================
print_status "TÃ©lÃ©chargement des modÃ¨les IA optimisÃ©s..."

# ModÃ¨le principal : Llama 3.2 3B (Ã©quilibre performance/qualitÃ©)
print_status "TÃ©lÃ©chargement Llama 3.2 3B (modÃ¨le principal)..."
ollama pull llama3.2:3b

# ModÃ¨le rapide : Llama 3.2 1B (ultra-rapide)
print_status "TÃ©lÃ©chargement Llama 3.2 1B (modÃ¨le rapide)..."
ollama pull llama3.2:1b

# ModÃ¨le spÃ©cialisÃ© : Qwen2.5-Coder (pour le code)
print_status "TÃ©lÃ©chargement Qwen2.5-Coder 1.5B (spÃ©cialisÃ© code)..."
ollama pull qwen2.5-coder:1.5b

print_success "ModÃ¨les IA tÃ©lÃ©chargÃ©s"

# =============================================================================
# 5. CRÃ‰ATION DE L'ENVIRONNEMENT NINA
# =============================================================================
print_status "CrÃ©ation de l'environnement Nina..."

# CrÃ©er le dossier Nina
mkdir -p /home/$USER/nina-ai
cd /home/$USER/nina-ai

# CrÃ©er l'environnement virtuel Python
python3 -m venv nina_env
source nina_env/bin/activate

# Installer les dÃ©pendances Python
pip install --upgrade pip

# DÃ©pendances Nina
pip install \
    ollama \
    requests \
    fastapi \
    uvicorn \
    websockets \
    python-multipart \
    jinja2 \
    psutil \
    schedule \
    colorama \
    rich

print_success "Environnement Nina crÃ©Ã©"

# =============================================================================
# 6. CRÃ‰ATION DE NINA INTELLIGENTE
# =============================================================================
print_status "CrÃ©ation de Nina IA intelligente..."

cat > nina_vm.py << 'EOF'
#!/usr/bin/env python3
"""
NINA IA - VERSION VM UBUNTU SÃ‰CURISÃ‰E
=====================================
Assistant IA local avec Ollama, optimisÃ© pour sÃ©curitÃ© et performance
"""

import os
import sys
import time
import json
import asyncio
from datetime import datetime
import ollama
import psutil
from rich.console import Console
from rich.panel import Panel
from rich.text import Text

class NinaVM:
    def __init__(self):
        self.console = Console()
        self.models = {
            'principal': 'llama3.2:3b',      # ModÃ¨le principal
            'rapide': 'llama3.2:1b',         # ModÃ¨le rapide
            'code': 'qwen2.5-coder:1.5b'     # SpÃ©cialisÃ© code
        }
        self.current_model = 'principal'
        self.conversation_history = []
        
        self.show_welcome()
        self.check_system()
    
    def show_welcome(self):
        """Affichage de bienvenue"""
        welcome_text = Text()
        welcome_text.append("ğŸ¤– NINA IA - VM UBUNTU SÃ‰CURISÃ‰E\n", style="bold blue")
        welcome_text.append("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", style="blue")
        welcome_text.append("âœ… 100% LOCAL - Vos donnÃ©es restent privÃ©es\n", style="green")
        welcome_text.append("âš¡ OLLAMA - IA puissante et rapide\n", style="green")
        welcome_text.append("ğŸ›¡ï¸ SÃ‰CURISÃ‰ - Isolation totale VM\n", style="green")
        welcome_text.append("ğŸ§  INTELLIGENT - ModÃ¨les multiples\n", style="green")
        
        panel = Panel(welcome_text, title="ğŸš€ NINA IA", border_style="blue")
        self.console.print(panel)
    
    def check_system(self):
        """VÃ©rifier l'Ã©tat du systÃ¨me"""
        self.console.print("ğŸ” VÃ©rification du systÃ¨me...", style="yellow")
        
        # VÃ©rifier Ollama
        try:
            models = ollama.list()
            available_models = [model['name'] for model in models['models']]
            
            self.console.print(f"âœ… Ollama actif - {len(available_models)} modÃ¨les disponibles", style="green")
            
            # VÃ©rifier les modÃ¨les requis
            for name, model in self.models.items():
                if model in available_models:
                    self.console.print(f"  âœ… {name}: {model}", style="green")
                else:
                    self.console.print(f"  âŒ {name}: {model} - Manquant", style="red")
            
        except Exception as e:
            self.console.print(f"âŒ Erreur Ollama: {e}", style="red")
            return False
        
        # VÃ©rifier les ressources systÃ¨me
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        self.console.print(f"ğŸ’¾ RAM: {memory.used//1024//1024//1024}GB/{memory.total//1024//1024//1024}GB utilisÃ©e", style="cyan")
        self.console.print(f"ğŸ”¥ CPU: {cpu_percent}% utilisÃ©", style="cyan")
        
        return True
    
    def switch_model(self, model_name):
        """Changer de modÃ¨le IA"""
        if model_name in self.models:
            self.current_model = model_name
            model = self.models[model_name]
            self.console.print(f"ğŸ”„ ModÃ¨le changÃ©: {model}", style="yellow")
            return True
        return False
    
    def generate_response(self, user_input, max_tokens=300):
        """GÃ©nÃ©rer une rÃ©ponse avec Ollama"""
        try:
            model = self.models[self.current_model]
            
            # Prompt systÃ¨me optimisÃ©
            system_prompt = """Tu es Nina, un assistant IA franÃ§ais intelligent et utile, inspirÃ© de Jarvis.

CaractÃ©ristiques :
- RÃ©ponds en franÃ§ais de maniÃ¨re naturelle et conversationnelle
- Sois prÃ©cis, informatif et crÃ©atif selon le contexte
- Adapte ton ton selon la situation (professionnel, amical, technique)
- Si tu ne sais pas quelque chose, dis-le honnÃªtement
- Reste concis mais complet dans tes rÃ©ponses

Tu fonctionnes entiÃ¨rement en local pour garantir la confidentialitÃ© des donnÃ©es."""

            # Construire le contexte avec historique
            context = ""
            if self.conversation_history:
                context = "\n".join(self.conversation_history[-3:])  # 3 derniers Ã©changes
            
            # Prompt complet
            full_prompt = f"{system_prompt}\n\nContexte rÃ©cent:\n{context}\n\nUtilisateur: {user_input}\nNina:"
            
            # GÃ©nÃ©rer la rÃ©ponse
            start_time = time.time()
            
            response = ollama.generate(
                model=model,
                prompt=full_prompt,
                options={
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'max_tokens': max_tokens,
                    'stop': ['Utilisateur:', 'Nina:']
                }
            )
            
            response_time = time.time() - start_time
            response_text = response['response'].strip()
            
            # Nettoyer la rÃ©ponse
            if response_text.startswith('Nina:'):
                response_text = response_text[5:].strip()
            
            # Ajouter Ã  l'historique
            self.conversation_history.append(f"User: {user_input[:50]}...")
            self.conversation_history.append(f"Nina: {response_text[:50]}...")
            
            # Limiter l'historique
            if len(self.conversation_history) > 10:
                self.conversation_history = self.conversation_history[-10:]
            
            return response_text, response_time
            
        except Exception as e:
            return f"Erreur lors de la gÃ©nÃ©ration: {e}", 0
    
    def show_help(self):
        """Afficher l'aide"""
        help_text = Text()
        help_text.append("ğŸ¤– COMMANDES NINA IA\n", style="bold blue")
        help_text.append("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n", style="blue")
        help_text.append("/help - Cette aide\n", style="cyan")
        help_text.append("/models - Lister les modÃ¨les disponibles\n", style="cyan")
        help_text.append("/switch <modÃ¨le> - Changer de modÃ¨le (principal/rapide/code)\n", style="cyan")
        help_text.append("/stats - Statistiques systÃ¨me\n", style="cyan")
        help_text.append("/clear - Effacer l'historique\n", style="cyan")
        help_text.append("/quit - Quitter Nina\n", style="cyan")
        help_text.append("\nTapez simplement votre question pour discuter !", style="green")
        
        panel = Panel(help_text, title="ğŸ“š Aide", border_style="blue")
        self.console.print(panel)
    
    def show_stats(self):
        """Afficher les statistiques systÃ¨me"""
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        stats_text = Text()
        stats_text.append(f"ğŸ’¾ RAM: {memory.used//1024//1024//1024}GB/{memory.total//1024//1024//1024}GB\n", style="cyan")
        stats_text.append(f"ğŸ”¥ CPU: {cpu_percent}%\n", style="cyan")
        stats_text.append(f"ğŸ¤– ModÃ¨le actuel: {self.models[self.current_model]}\n", style="yellow")
        stats_text.append(f"ğŸ’¬ Historique: {len(self.conversation_history)} entrÃ©es\n", style="green")
        
        panel = Panel(stats_text, title="ğŸ“Š Statistiques", border_style="cyan")
        self.console.print(panel)
    
    def run(self):
        """Boucle principale"""
        self.console.print("\nğŸš€ Nina IA est prÃªte ! Tapez /help pour voir les commandes.\n", style="bold green")
        
        while True:
            try:
                # Prompt utilisateur avec modÃ¨le actuel
                model_indicator = f"[{self.current_model}]"
                user_input = input(f"ğŸ‘¤ Vous {model_indicator}: ").strip()
                
                if not user_input:
                    continue
                
                # Commandes spÃ©ciales
                if user_input.startswith('/'):
                    cmd = user_input[1:].lower().split()
                    
                    if cmd[0] == 'help':
                        self.show_help()
                    elif cmd[0] == 'quit':
                        self.console.print("ğŸ‘‹ Au revoir ! Nina s'arrÃªte.", style="yellow")
                        break
                    elif cmd[0] == 'clear':
                        self.conversation_history.clear()
                        self.console.print("ğŸ§¹ Historique effacÃ©", style="green")
                    elif cmd[0] == 'stats':
                        self.show_stats()
                    elif cmd[0] == 'models':
                        self.console.print("ğŸ¤– ModÃ¨les disponibles:", style="blue")
                        for name, model in self.models.items():
                            indicator = "ğŸ‘ˆ" if name == self.current_model else "  "
                            self.console.print(f"  {indicator} {name}: {model}", style="cyan")
                    elif cmd[0] == 'switch' and len(cmd) > 1:
                        if self.switch_model(cmd[1]):
                            self.console.print(f"âœ… ModÃ¨le changÃ©: {cmd[1]}", style="green")
                        else:
                            self.console.print(f"âŒ ModÃ¨le inconnu: {cmd[1]}", style="red")
                    else:
                        self.console.print("âŒ Commande inconnue. Tapez /help", style="red")
                    
                    continue
                
                # GÃ©nÃ©ration de rÃ©ponse
                self.console.print("ğŸ¤” Nina rÃ©flÃ©chit...", style="yellow")
                response, response_time = self.generate_response(user_input)
                
                # Affichage de la rÃ©ponse
                response_text = Text()
                response_text.append("ğŸ¤– Nina: ", style="bold blue")
                response_text.append(response, style="white")
                response_text.append(f" (â±ï¸ {response_time:.1f}s)", style="dim")
                
                self.console.print(response_text)
                self.console.print()  # Ligne vide
                
            except KeyboardInterrupt:
                self.console.print("\nğŸ‘‹ Au revoir ! Nina s'arrÃªte (Ctrl+C).", style="yellow")
                break
            except Exception as e:
                self.console.print(f"âŒ Erreur: {e}", style="red")

if __name__ == "__main__":
    try:
        nina = NinaVM()
        nina.run()
    except Exception as e:
        print(f"âŒ Erreur fatale: {e}")
        sys.exit(1)
EOF

chmod +x nina_vm.py
print_success "Nina IA crÃ©Ã©e"

# =============================================================================
# 7. CRÃ‰ATION DU SERVICE SYSTEMD (OPTIONNEL)
# =============================================================================
print_status "CrÃ©ation du service Nina (optionnel)..."

sudo tee /etc/systemd/system/nina.service > /dev/null << EOF
[Unit]
Description=Nina IA Assistant
After=network.target ollama.service

[Service]
Type=simple
User=$USER
WorkingDirectory=/home/$USER/nina-ai
Environment=PATH=/home/$USER/nina-ai/nina_env/bin
ExecStart=/home/$USER/nina-ai/nina_env/bin/python /home/$USER/nina-ai/nina_vm.py
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

print_success "Service Nina crÃ©Ã©"

# =============================================================================
# 8. CONFIGURATION SÃ‰CURITÃ‰
# =============================================================================
print_status "Configuration sÃ©curitÃ©..."

# Firewall - Bloquer tout sauf SSH
sudo ufw --force enable
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow ssh

# DÃ©sactiver les services inutiles
sudo systemctl disable bluetooth 2>/dev/null || true
sudo systemctl disable cups 2>/dev/null || true

print_success "SÃ©curitÃ© configurÃ©e"

# =============================================================================
# 9. SCRIPTS UTILITAIRES
# =============================================================================
print_status "CrÃ©ation des scripts utilitaires..."

# Script de dÃ©marrage rapide
cat > start_nina.sh << 'EOF'
#!/bin/bash
cd /home/$USER/nina-ai
source nina_env/bin/activate
python nina_vm.py
EOF
chmod +x start_nina.sh

# Script de test systÃ¨me
cat > test_system.sh << 'EOF'
#!/bin/bash
echo "ğŸ” Test du systÃ¨me Nina IA"
echo "========================="

echo "1. Test Ollama..."
ollama list

echo -e "\n2. Test Python..."
source nina_env/bin/activate
python -c "import ollama; print('âœ… Ollama Python OK')"

echo -e "\n3. Test modÃ¨les..."
python -c "
import ollama
models = ollama.list()
print(f'ModÃ¨les disponibles: {len(models[\"models\"])}')
for model in models['models']:
    print(f'  - {model[\"name\"]}')
"

echo -e "\n4. Ressources systÃ¨me..."
free -h
df -h /
EOF
chmod +x test_system.sh

print_success "Scripts utilitaires crÃ©Ã©s"

# =============================================================================
# 10. FINALISATION
# =============================================================================
print_success "ğŸ‰ INSTALLATION NINA IA TERMINÃ‰E !"
echo ""
echo "ğŸ“‹ RÃ‰SUMÃ‰ DE L'INSTALLATION :"
echo "============================="
echo "âœ… Ubuntu mis Ã  jour"
echo "âœ… Ollama installÃ© et configurÃ©"
echo "âœ… ModÃ¨les IA tÃ©lÃ©chargÃ©s (Llama 3.2 3B, 1B, Qwen2.5-Coder)"
echo "âœ… Nina IA crÃ©Ã©e et optimisÃ©e"
echo "âœ… Environnement Python configurÃ©"
echo "âœ… SÃ©curitÃ© configurÃ©e (UFW, services)"
echo "âœ… Scripts utilitaires crÃ©Ã©s"
echo ""
echo "ğŸš€ POUR DÃ‰MARRER NINA :"
echo "======================="
echo "cd /home/$USER/nina-ai"
echo "./start_nina.sh"
echo ""
echo "ğŸ”§ POUR TESTER LE SYSTÃˆME :"
echo "=========================="
echo "./test_system.sh"
echo ""
echo "ğŸ›¡ï¸ SÃ‰CURITÃ‰ :"
echo "============"
echo "- Firewall activÃ© (SSH uniquement)"
echo "- VM isolÃ©e du rÃ©seau principal"
echo "- DonnÃ©es 100% locales"
echo "- Services inutiles dÃ©sactivÃ©s"
echo ""
print_success "Nina IA est prÃªte Ã  l'emploi ! ğŸ¤–"
EOF

chmod +x nina_vm_setup.sh 